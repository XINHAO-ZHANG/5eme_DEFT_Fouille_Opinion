{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c1f8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3a76b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_brut(file_path,output_csv_path):\n",
    "    '''\n",
    "    extraire les textes et leurs parities à partir des corpus d'apprentissage bruts\n",
    "    et les conserver dans un fichier csv pour chaque langue\n",
    "    '''    \n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "    for doc in root.findall('.//doc'):\n",
    "        # obtenir tous les paragraphes du texte et les joindre en un seul\n",
    "        text_elements = doc.findall('.//texte//p')\n",
    "        texte = ' '.join([elem.text.strip() for elem in text_elements if elem.text])\n",
    "        partis = [parti.get('valeur') for parti in doc.findall('.//PARTI')]\n",
    "        for parti in partis:\n",
    "            data.append({'texte': texte, 'parti': parti})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df.to_csv(output_csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89143407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine_text_label(xml_file_path, label_file_path, output_csv_path):\n",
    "    '''\n",
    "    extraire les textes tests dans les fichiers xml de et leurs labels des fichiers textes \n",
    "    et les combiner dans un seul fichier csv pour chaque langue\n",
    "    '''   \n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    test_data = []\n",
    "    for doc in root.findall('.//doc'):\n",
    "        # obtenir tous les paragraphes du texte et les joindre en un seul\n",
    "        text_elements = doc.findall('.//texte//p')\n",
    "        texte = ' '.join([elem.text.strip() for elem in text_elements if elem.text])\n",
    "        test_data.append({'texte': texte})\n",
    "\n",
    "    texte_df = pd.DataFrame(test_data)\n",
    "\n",
    "    # lecture des labels à partir du fichier texte et les combiner avec les textes dans un même dataframe\n",
    "    labels_df = pd.read_csv(label_file_path, sep='\\t', header=None, names=['id', 'label'])\n",
    "    labels_df.set_index('id', inplace=True)\n",
    "\n",
    "    # ajouter une colonne id pour pouvoir joindre les deux dataframes\n",
    "    texte_df['id'] = range(1, len(texte_df) + 1)\n",
    "    texte_df.set_index('id', inplace=True)\n",
    "\n",
    "    # joindre les deux dataframes en utilisant l'index id comme clé de jointure \n",
    "    combined_df = texte_df.join(labels_df, how='left')\n",
    "    return combined_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26c6295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data_brut(\"corpus/deft09_parlement_appr_xml/deft09_parlement_appr_en.xml\",\"corpus/output_en_app.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77a3744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e04092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiaohua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xiaohua/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# installer les packages nltk nécessaires pour la préparation des données \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f782b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lang):\n",
    "    '''\n",
    "    prétraiter le texte en appliquant les étapes suivantes:\n",
    "    - convertir le texte en minuscules\n",
    "    - supprimer les nombres\n",
    "    - supprimer les apostrophes\n",
    "    - supprimer les mots de longueur inférieure ou égale à 2\n",
    "    - supprimer les stopwords et la ponctuation\n",
    "    '''\n",
    "    # assurer que le texte est de type str\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)  # convertir en str\n",
    "\n",
    "    # charger les stopwords selon la langue\n",
    "    if lang == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    elif lang == 'fr':\n",
    "        stop_words = set(stopwords.words('french'))\n",
    "    elif lang == 'it':\n",
    "        stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "    # enlever les symboles ennuyeux et les nombres \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r\"['’‘]\", ' ', text)  \n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  \n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # enlever les stopwords et la ponctuation\n",
    "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20bb2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(csv_path, lang,output_csv_path):\n",
    "    '''\n",
    "    lecture du fichier csv contenant les textes et leurs labels\n",
    "    et prétraitement des textes\n",
    "    '''\n",
    "    # lire le fichier csv sauvegardé précédemment\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # appliquer la fonction de prétraitement sur la colonne texte\n",
    "    df['texte'] = df['texte'].apply(lambda x: preprocess_text(x, lang))\n",
    "\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6270545",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_csv(\"/Users/xiaohua/Desktop/Cours/M2_Paris/Apprentissage_auto/proj_futo/corpus/output_en_app.csv\",\"en\",\"/Users/xiaohua/Desktop/Cours/M2_Paris/Apprentissage_auto/proj_futo/corpus/output_en_app_traite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cac285b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67736e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfVectorize(CSV_train, CSV_test):\n",
    "    '''\n",
    "    vectoriser les textes en utilisant la méthode TF-IDF\n",
    "    '''\n",
    "    train_df = pd.read_csv(CSV_train)\n",
    "    test_df = pd.read_csv(CSV_test)\n",
    "    train_df.dropna(subset=['texte'], inplace=True)\n",
    "    test_df.dropna(subset=['texte'], inplace=True)\n",
    "\n",
    "\n",
    "    # initialiser le vectorizer avec les paramètres désirés\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_df=0.5,\n",
    "        use_idf=True,\n",
    "        sublinear_tf=True,\n",
    "        max_features=10000\n",
    "    )\n",
    "\n",
    "    # vectoriser les textes d'entraînement et de test \n",
    "    print(\"Vectorizing train data...\")\n",
    "    X_train = vectorizer.fit_transform(tqdm(train_df['texte']))\n",
    "\n",
    "    print(\"Vectorizing test data...\")\n",
    "    X_test = vectorizer.transform(tqdm(test_df['texte']))\n",
    "\n",
    "    # récupérer les labels correspondants\n",
    "    y_train = train_df['parti'].astype(str)\n",
    "    y_test = test_df['parti'].astype(str)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5371733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/19365 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                   | 639/19365 [00:00<00:02, 6378.08it/s]\u001b[A\n",
      "  7%|██▎                                 | 1277/19365 [00:00<00:03, 5897.48it/s]\u001b[A\n",
      " 10%|███▍                                | 1870/19365 [00:00<00:03, 5673.01it/s]\u001b[A\n",
      " 13%|████▌                               | 2439/19365 [00:00<00:03, 5511.49it/s]\u001b[A\n",
      " 15%|█████▌                              | 2991/19365 [00:00<00:03, 5155.96it/s]\u001b[A\n",
      " 18%|██████▌                             | 3510/19365 [00:00<00:03, 4973.07it/s]\u001b[A\n",
      " 21%|███████▍                            | 4010/19365 [00:00<00:03, 4923.37it/s]\u001b[A\n",
      " 23%|████████▎                           | 4504/19365 [00:00<00:03, 4831.69it/s]\u001b[A\n",
      " 26%|█████████▎                          | 4988/19365 [00:00<00:03, 4773.71it/s]\u001b[A\n",
      " 28%|██████████▏                         | 5466/19365 [00:01<00:02, 4719.88it/s]\u001b[A\n",
      " 31%|███████████                         | 5938/19365 [00:01<00:03, 4295.54it/s]\u001b[A\n",
      " 33%|███████████▉                        | 6411/19365 [00:01<00:02, 4411.61it/s]\u001b[A\n",
      " 35%|████████████▊                       | 6874/19365 [00:01<00:02, 4467.24it/s]\u001b[A\n",
      " 38%|█████████████▋                      | 7338/19365 [00:01<00:02, 4515.32it/s]\u001b[A\n",
      " 40%|██████████████▍                     | 7799/19365 [00:01<00:02, 4541.79it/s]\u001b[A\n",
      " 43%|███████████████▎                    | 8256/19365 [00:01<00:02, 4424.07it/s]\u001b[A\n",
      " 45%|████████████████▏                   | 8701/19365 [00:01<00:02, 4406.58it/s]\u001b[A\n",
      " 47%|█████████████████                   | 9153/19365 [00:01<00:02, 4438.42it/s]\u001b[A\n",
      " 50%|█████████████████▊                  | 9598/19365 [00:02<00:02, 4418.02it/s]\u001b[A\n",
      " 52%|██████████████████▏                | 10041/19365 [00:02<00:02, 4413.85it/s]\u001b[A\n",
      " 54%|██████████████████▉                | 10483/19365 [00:02<00:02, 4397.29it/s]\u001b[A\n",
      " 56%|███████████████████▋               | 10924/19365 [00:02<00:01, 4365.79it/s]\u001b[A\n",
      " 59%|████████████████████▌              | 11361/19365 [00:02<00:01, 4312.01it/s]\u001b[A\n",
      " 61%|█████████████████████▎             | 11793/19365 [00:02<00:01, 4307.07it/s]\u001b[A\n",
      " 63%|██████████████████████             | 12241/19365 [00:02<00:01, 4356.09it/s]\u001b[A\n",
      " 65%|██████████████████████▉            | 12677/19365 [00:02<00:01, 3505.32it/s]\u001b[A\n",
      " 68%|███████████████████████▋           | 13126/19365 [00:02<00:01, 3753.08it/s]\u001b[A\n",
      " 70%|████████████████████████▍          | 13544/19365 [00:03<00:01, 3866.63it/s]\u001b[A\n",
      " 72%|█████████████████████████▏         | 13949/19365 [00:03<00:01, 3909.83it/s]\u001b[A\n",
      " 74%|█████████████████████████▉         | 14381/19365 [00:03<00:01, 4023.21it/s]\u001b[A\n",
      " 76%|██████████████████████████▊        | 14808/19365 [00:03<00:01, 4090.75it/s]\u001b[A\n",
      " 79%|███████████████████████████▌       | 15244/19365 [00:03<00:00, 4168.10it/s]\u001b[A\n",
      " 81%|████████████████████████████▎      | 15675/19365 [00:03<00:00, 4207.04it/s]\u001b[A\n",
      " 83%|█████████████████████████████      | 16100/19365 [00:03<00:00, 4214.04it/s]\u001b[A\n",
      " 85%|█████████████████████████████▊     | 16524/19365 [00:03<00:00, 4207.57it/s]\u001b[A\n",
      " 88%|██████████████████████████████▋    | 16947/19365 [00:03<00:00, 4163.98it/s]\u001b[A\n",
      " 90%|███████████████████████████████▍   | 17369/19365 [00:03<00:00, 4177.69it/s]\u001b[A\n",
      " 92%|████████████████████████████████▏  | 17788/19365 [00:04<00:00, 4109.51it/s]\u001b[A\n",
      " 94%|████████████████████████████████▉  | 18200/19365 [00:04<00:00, 4062.56it/s]\u001b[A\n",
      " 96%|█████████████████████████████████▋ | 18609/19365 [00:04<00:00, 4069.57it/s]\u001b[A\n",
      "100%|███████████████████████████████████| 19365/19365 [00:04<00:00, 4362.90it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/12911 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▌                                   | 534/12911 [00:00<00:02, 5337.65it/s]\u001b[A\n",
      "  9%|███▏                                | 1123/12911 [00:00<00:02, 5661.51it/s]\u001b[A\n",
      " 13%|████▋                               | 1699/12911 [00:00<00:01, 5705.66it/s]\u001b[A\n",
      " 18%|██████▍                             | 2296/12911 [00:00<00:01, 5807.45it/s]\u001b[A\n",
      " 22%|████████                            | 2877/12911 [00:00<00:01, 5770.33it/s]\u001b[A\n",
      " 27%|█████████▋                          | 3455/12911 [00:00<00:01, 5757.18it/s]\u001b[A\n",
      " 31%|███████████▏                        | 4031/12911 [00:00<00:01, 5688.15it/s]\u001b[A\n",
      " 36%|████████████▉                       | 4639/12911 [00:00<00:01, 5810.12it/s]\u001b[A\n",
      " 41%|██████████████▋                     | 5246/12911 [00:00<00:01, 5887.85it/s]\u001b[A\n",
      " 45%|████████████████▎                   | 5836/12911 [00:01<00:01, 5556.16it/s]\u001b[A\n",
      " 50%|█████████████████▊                  | 6396/12911 [00:01<00:01, 5471.44it/s]\u001b[A\n",
      " 54%|███████████████████▍                | 6983/12911 [00:01<00:01, 5581.41it/s]\u001b[A\n",
      " 59%|█████████████████████               | 7560/12911 [00:01<00:00, 5633.83it/s]\u001b[A\n",
      " 63%|██████████████████████▋             | 8126/12911 [00:01<00:00, 5618.73it/s]\u001b[A\n",
      " 67%|████████████████████████▏           | 8691/12911 [00:01<00:00, 5620.55it/s]\u001b[A\n",
      " 72%|█████████████████████████▊          | 9271/12911 [00:01<00:00, 5672.94it/s]\u001b[A\n",
      " 76%|███████████████████████████▍        | 9839/12911 [00:01<00:00, 5554.71it/s]\u001b[A\n",
      " 81%|████████████████████████████▏      | 10396/12911 [00:01<00:00, 5506.35it/s]\u001b[A\n",
      " 85%|█████████████████████████████▋     | 10958/12911 [00:01<00:00, 5538.48it/s]\u001b[A\n",
      " 89%|███████████████████████████████▏   | 11513/12911 [00:02<00:00, 5413.16it/s]\u001b[A\n",
      " 94%|████████████████████████████████▋  | 12074/12911 [00:02<00:00, 5467.39it/s]\u001b[A\n",
      "100%|███████████████████████████████████| 12911/12911 [00:02<00:00, 5586.45it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<19365x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2517771 stored elements in Compressed Sparse Row format>,\n",
       " <12911x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1662430 stored elements in Compressed Sparse Row format>,\n",
       " 0          GUE-NGL\n",
       " 1           PPE-DE\n",
       " 2           PPE-DE\n",
       " 3           PPE-DE\n",
       " 4              PSE\n",
       "            ...    \n",
       " 19361         ELDR\n",
       " 19362       PPE-DE\n",
       " 19363    Verts-ALE\n",
       " 19364         ELDR\n",
       " 19365         ELDR\n",
       " Name: parti, Length: 19365, dtype: object,\n",
       " 0              PSE\n",
       " 1           PPE-DE\n",
       " 2        Verts-ALE\n",
       " 3           PPE-DE\n",
       " 4             ELDR\n",
       "            ...    \n",
       " 12908       PPE-DE\n",
       " 12909         ELDR\n",
       " 12910          PSE\n",
       " 12911          PSE\n",
       " 12912    Verts-ALE\n",
       " Name: parti, Length: 12911, dtype: object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVectorize(\"/Users/xiaohua/Desktop/Cours/M2_Paris/Apprentissage_auto/5eme_DEFT_Fouille_Opinion/data/texte_cleaned/train_text_cleaned_en.csv\",\"/Users/xiaohua/Desktop/Cours/M2_Paris/Apprentissage_auto/5eme_DEFT_Fouille_Opinion/data/texte_cleaned/test_text_cleaned_en.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44098dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from sklearn.metrics import classification_report, accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4228b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, X_test, y_train, y_test, model):\n",
    "    '''\n",
    "    Évaluer les performances du modèle choisi\n",
    "    en calculant l'accuracy, le kappa et la matrice de confusion\n",
    "    '''\n",
    "    # Entraîner le modèle sur les données d'entraînement\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire les labels sur les données de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64dd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2263d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(y_test, y_pred):\n",
    "    '''\n",
    "    Afficher les scores d'évaluation du modèle choisi\n",
    "    '''\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Kappa Score:\", kappa)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dee597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test,y_pred, model):\n",
    "    '''\n",
    "    Sauvagarder la visualisation de la matrice de confusion du modèle choisi\n",
    "    '''\n",
    "    # obtenir les labels prédits par le modèle\n",
    "    unique_labels = sorted(set(y_test) | set(y_pred))\n",
    "\n",
    "    # générer la matrice de confusion en spécifiant les labels\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
    "\n",
    "    # utiliser seaborn pour afficher la matrice de confusion sous forme de heatmap\n",
    "    plt.figure(figsize=(6, 4.5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greys', \n",
    "                xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "\n",
    "    plt.xlabel('Parti Prédit')\n",
    "    plt.ylabel('Parti Réel')\n",
    "    plt.title('Matrice de Confusion')\n",
    "\n",
    "    # sauvegarder la figure dans le dossier result\n",
    "    plt.savefig(f'../result/Confusion_Matrix_{model}.png', format='png', dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89c921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
